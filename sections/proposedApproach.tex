\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

Our method is focused on inpainting a scarce sinogram using information from a shape prior, instead of an optimisation process. As such, it requires an initial training of the generative model, in our case a GAN, followed by the inpainting process. The training procedure is visually detailed in Fig. \ref{fig:training_procedure_explanation}.

\subsubsection{GAN introduction}
A GAN is made of two networks that compete against each other. The generator \textit{G} tries to generate samples that follow the same distribution $p_{data}$ as the training examples by up-sampling a noise vector drawn from a noise distribution $p_{z}$. The discriminator \textit{D}, tries to discriminate between  real training samples and those generated by \textit{G}. GANs have been proposed as an alternative to avoid difficulties commonly found in deep generative models, such as explicit density estimation. GAN optimisation is done by solving the minimax problem: 
\begin{align*}
\label{GAN_Loss}
\min_G \max_D \mathcal{L}(\mathit{G}, \mathit{D}) &= \mathbb{E}_{x \sim p_{data}} \log(D(x)) +\\
& \mathbb{E}_{z \sim p_{z}} \log(1-D(G(z)))
\end{align*}
where $\mathbb{E}$ is the expectation over the training dataset. This architecture uses fully-connected units, which limit the maximum size of images the GAN can generate. To scale to larger images, the Deep Convolutional GAN (DCGAN) architecture \cite{radford2015unsupervised} was proposed. 

\subsection{Deep Convolutional GAN and Unet - The pix2pix Architecture}

GAN are notoriously difficult to train \cite{goodfellow2016nips} and do not handle prior-knowledge as they sample a latent distribution. To address these issues, \cite{isola2017image} proposes the pix2pix architecture that combines the DCGAN improvements and includes shape priors by changing the generator into a U-net architecture.

The main change offered by the DCGAN is the replacement of fully-connected units by convolutional layers. Other modifications include the replacement of the maxout \cite{goodfellow2013maxout} activation by ReLu and Tanh in the generator and LeakyReLu in the discriminator, the inclusion of batch-normalisation \cite{ioffe2015batch} and the replacement of pooling units with learnt sampling units. 

U-nets were initially designed for image segmentation. They are fully convolutional auto-encoders that have residual connections between the down-sampling and up-sampling units. They are convenient architectures that can map a multi-channel input image to a one-channel output image, or vice versa. The pix2pix architecture makes use of this design to add prior-knowledge to the GAN. 

\subsection{Adapted Loss Function}

To encourage the generator to produce images close to their target value, in the sense of the L1-loss, an additional term is added to the training loss as prescribed in \cite{pathak2016context}. We use the L1-Loss  which is a good choice for image quality when Poisson noise is present, as is the case in XCT images \cite{dupe2011inverse}. Unlike other inpainting processes, we train the network to infer only the missing part of the image. We thus use the loss function:
\begin{align}
\min_G \max_D \mathcal{L}(\mathit{G}, \mathit{D}) &= \mathbb{E}_{x \sim p_{true}} \log(D(x)) +  \\
& \mathbb{E}_{\tilde{x} \sim p_{impaired}} \log(1-D(G( \tilde{x}, y ))) + \\
& \lambda L_1 (G( \tilde{x}, y ), x) 
\end{align}
 
where $x$ and $\tilde{x}$ are the  sinograms with all and missing data respectively, $y$ is an image that encodes the shape prior and $\lambda$ a weighting parameter. Given a sinogram and a shape prior, $\mathit{D}$ determines if the sinogram was generated by the GAN or drawn from the true dataset. Given a sinogram with missing data, $\mathit{G}$ generates missing acquisitions that are close to the ground-truth acquisitions in terms of L1-Loss\cite{larsen2016autoencoding}.

\subsection{Architectural Details}
We follow the network's description given in \cite{isola2017image}. Let Ck be a Convolution-BatchNorm-ReLU layer with k filters. Let CDk be a Convolution-BatchNorm-Dropout-ReLU layer with a dropout rate of 50\% \cite{srivastava2014dropout}. All convolutions are 4 by 4 spatial filters applied with a stride of 2. Convolutions in the encoder down-sample by a factor of 2, whereas in the decoder they up-sample by a factor of 2. After the last layer in the decoder, a convolution is applied, followed by a Tanh function and an element-wise multiplication with a mask to infer missing acquisitions only, as explained in \ref{encoding_the_shape_prior}. As an exception to the above notation, BatchNorm is not applied to the first C64 layer in the encoder. All ReLUs \cite{nair2010rectified} in the encoder are leaky \cite{maas2013rectifier}, with slope 0.2, while ReLUs in the decoder are not leaky. This results in the following generator:

\textbf{encoder:}

C64-C128-C256-C512-C512-C512-C512-C512

\textbf{decoder:}

CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128

The discriminator works on image patches of size 16x16. For the discriminator, after its last layer and after extracting the patch, a convolution is applied to map to a one-channel output, followed by a Sigmoid function. This results in the following discriminator:

\textbf{encoder:}

C64-C128-C256-C512

\subsection{The Inpainting Process}
Once the network is trained and its parameters frozen, the inpainting process is straightforward. It requires a scarce sinogram, a sufficiently sampled CAD sinogram and a mask indicating the acquisitions to infer. Given this input data, $\mathit{G}$ infers the missing acquisitions in a one-channel image. This image is then added pixel-wise to the scarce sinogram to produce the inpainted sinogram. A visual explanation of the method is given in Fig. \ref{fig:inpainting_method_explanation}.

\subsection{Other Methods for Comparison}
An immediate solution to inferring missing acquisitions is to use linear interpolation. Using the shape prior, replacing the missing acquisitions by the ones expected by the CAD is another solution. An improvement of the acquisition prior is brought by scaling it with attenuation values matching the ones seen in the measured sinogram: this scaling is detailed in \ref{encoding_the_shape_prior}. We here implement these three solutions for comparison.

The pix2pix architecture used here has never been used for XCT data but other similar methods exist. Indeed, \cite{lee2018deep} replaces missing acquisitions by linearly interpolated ones and then uses a Unet \cite{ronneberger2015u} to enhance their quality. We implement this architecture without the shape prior as in the original paper and also produce a version that includes our shape prior to make it more comparable to our new approach. A GAN architecture is also used for sinogram synthesis \cite{yoo2019sinogram}, with an optimisation procedure to perform the inpainting. This model does not use shape priors. We implemented this architecture and trained it with as much care as we trained the other, but could not obtain a satisfactory result. 

\end{document}