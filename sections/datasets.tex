\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

We tackle the sinogram inpainting with shape information problem using a data-driven method, that relies on large datasets. However, such datasets of XCT acquisitions with associated shape priors do not exist. We therefore adapted the SophiaBeads \cite{coban2015sophiabeads_dataset} dataset to report the performance of our process on real data. 

\subsection{Preprocessing the SophiaBeads Dataset}
Initially designed to provide data for researchers in XCT reconstruction to benchmark their algorithms, this dataset is a collection of six cone beam XCT acquisitions of a plastic tube filled with soda-lime glass beads. The difference between each dataset is the number of projections per volume. As the beads are approximately circular, each slice through the volume contains only circular objects.

We reconstruct two high-projections datasets using the approach in \cite{coban2015sophiabeads_codes} and \cite{biguri2016tigre}, generating 256x256x200 volumes. For experimental convenience and speed, training data is generated from individual 2D slices, using a parallel beam geometry. Circle finding methods are used to identify object boundaries, which were used as shape priors.  

Thus, our datasets consist of two volumes of 200 slices each, together with the associated CAD data. Each volume is split into 180 training slices and 20 test slices. To test the robustness of our method, the outline of the plastic container was not captured in the CAD data and several small circles are also not included. This is done to ensure that the images had features that are not represented in the CAD data. A slice of the volume reconstructed with 512 projections and the identified beads, from which the object boundaries can be inferred are shown in Fig. \ref{fig:sophiabeads_obj_slice} and Fig. \ref{fig:sophiabeads_cad_slice}, respectively.

We generate full sinograms from each slice with \cite{biguri2016tigre}, simulating a detector with 256 pixels and measuring from 256 equally spaced angles in a 180 degree arc. The sinograms corresponding to Fig. \ref{fig:sophiabeads_obj_slice} and Fig. \ref{fig:sophiabeads_cad_slice} are shown in Fig. \ref{fig:sophiabeads_obj_sin} and Fig. \ref{fig:sophiabeads_cad_sin}, respectively. Sinograms with missing projections are generated by setting between 5 to 95\% of consecutive angular measurements to 0. It is worth emphasizing that we did not remove scan directions uniformly at random, but choose the more challenging setting where larger numbers of consecutive angles are missing. An example of a sinogram with half of its data missing is shown in Fig. \ref{fig:sophia_sin_example}.

\subsection{Synthetic Training Data}
\label{synthetic_training_data}

Having a synthetic dataset allows experimenting with the data model. In real applications, the shape prior will not always exactly match the object shape. There could be boundary deviations, or objects could have unknown inclusions and internal defects. Therefore, we also generate separate data with additional air pockets in the objects that were not captured by the shape prior, as can be seen in Fig. \ref{fig:holes_examples}. 

In order to simulate such defects, we create a synthetic dataset to mirror the SophiaBeads data, as shown in Fig. \ref{fig:syn_obj_slice} and Fig. \ref{fig:syn_cad_slice}. Each slice is a 256 by 256 image of several circles generated with the \textit{circle} function from the scikit-image.draw Python \cite{van2014scikit} module with different radii and positions and uniform densities with added Gaussian noise. Object attenuation values are set to 25 while the image background is assumed to have an X-Ray attenuation much lower than the object and is set to 1. The shape prior associated to a slice is generated using the same function with the same radii and position for each circle but with different densities and no Gaussian noise. Whilst we chose this model to replicate the SophiaBeads dataset, similar data has previously been used in \cite{jin2017deep}, \cite{adler2017solving} and \cite{kelly2017deep}.

\end{document}